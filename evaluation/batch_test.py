import os
import re
import sys
import json
import torch
import argparse
import logging
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from datasets import load_dataset, Dataset

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä¾¿äºå¯¼å…¥å…¶ä»–æ¨¡å—
sys.path.append("/root/autodl-tmp/GRPO_MATH")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestConfig:
    """æ‰¹é‡æµ‹è¯•é…ç½®ç±»"""
    # è·¯å¾„é…ç½®
    base_model_path: str = "/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B"
    model_path: str = "./lora_finetuned_qwen"  # å¯ä»¥æ˜¯LoRAæˆ–GRPOçš„æ¨¡å‹è·¯å¾„
    dataset_path: str = "/root/autodl-tmp/GRPO_MATH/gsm8k"
    output_dir: str = "./evaluation_results"
    
    # æµ‹è¯•å‚æ•°
    max_length: int = 512
    sample_size: int = 100
    batch_size: int = 8
    temperature: float = 0.1
    max_new_tokens: int = 256
    
    # è¯„ä¼°æ¨¡å¼
    model_type: str = "lora"  # 'lora' æˆ– 'grpo'
    
    # å…¶ä»–é…ç½®
    use_bf16: bool = True
    
    def __post_init__(self):
        """åå¤„ç†é…ç½®"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)


class GSM8KDataHandler:
    """GSM8Kæ•°æ®é›†å¤„ç†å™¨ - ä»åŸæ–‡ä»¶å¤ç”¨"""
    
    def __init__(self, config: TestConfig, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.test_data = None
    
    def extract_answer_from_text(self, answer_text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å¯»æ‰¾ #### åé¢çš„æ•°å­—
        pattern = r'####\s*(-?\d+(?:\.\d+)?)'
        match = re.search(pattern, answer_text)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°####ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        
        return None
    
    def has_correct_format(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ­£ç¡®çš„æ ¼å¼"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å« #### æ ‡è®°
        has_marker = "####" in text
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªæ¨ç†æ­¥éª¤ï¼ˆè‡³å°‘æœ‰3è¡Œæ–‡æœ¬ï¼‰
        has_steps = len(text.split('\n')) >= 3
        
        return has_marker and has_steps
        
    def load_test_data(self) -> List[Dict]:
        """åŠ è½½GSM8Kæµ‹è¯•æ•°æ®"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæµ‹è¯•æ•°æ®...")
        
        try:
            # åŠ è½½Parquetæ ¼å¼
            test_parquet_path = os.path.join(self.config.dataset_path, "main/test-00000-of-00001.parquet")
            
            if os.path.exists(test_parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®")
                dataset = load_dataset("parquet", data_files={"test": test_parquet_path})
                self.test_data = list(dataset['test'])
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶: {test_parquet_path}")
                
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            raise
        
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(self.test_data)}")
        return self.test_data
    
    def get_test_samples(self, n_samples: Optional[int] = None) -> List[Dict]:
        """è·å–æµ‹è¯•æ ·æœ¬"""
        if self.test_data is None:
            self.load_test_data()
            
        if n_samples is None:
            return self.test_data
        return self.test_data[:min(n_samples, len(self.test_data))]
    
    def create_prompts(self, test_samples: List[Dict]) -> List[str]:
        """ä¸ºæµ‹è¯•æ ·æœ¬åˆ›å»ºæç¤º"""
        prompts = []
        
        for sample in test_samples:
            # å°è¯•ä½¿ç”¨chat templateæ ¼å¼åŒ–
            try:
                conversation = [
                    {"role": "user", "content": "Please solve this math problem step by step and provide your final answer after the \"####\" marker.\n"+sample['question']},
                ]
                prompt = self.tokenizer.apply_chat_template(
                    conversation,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except Exception as e:
                logger.warning(f"åº”ç”¨chat templateå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•æ ¼å¼")
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                prompt = f"Please solve this math problem step by step and provide your final answer after the \"####\" marker.\nQuestion: {sample['question']}\nAnswer: "
            
            prompts.append(prompt)
        
        return prompts


class ModelTester:
    """æ¨¡å‹æ‰¹é‡æµ‹è¯•å™¨"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.data_handler = None
    
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model_path}")
        logger.info(f"åŠ è½½å¾®è°ƒæ¨¡å‹: {self.config.model_path}")
        
        # å…ˆå°è¯•ä»å¾®è°ƒæ¨¡å‹ç›®å½•åŠ è½½åˆ†è¯å™¨
        try:
            logger.info(f"å°è¯•ä»å¾®è°ƒæ¨¡å‹ç›®å½•åŠ è½½åˆ†è¯å™¨")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path, 
                trust_remote_code=True, 
                padding_side='left'
            )
            logger.info("æˆåŠŸä»å¾®è°ƒæ¨¡å‹ç›®å½•åŠ è½½åˆ†è¯å™¨")
        except Exception as e:
            logger.warning(f"ä»å¾®è°ƒæ¨¡å‹ç›®å½•åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œä»åŸºç¡€æ¨¡å‹åŠ è½½
            logger.info(f"ä»åŸºç¡€æ¨¡å‹åŠ è½½åˆ†è¯å™¨")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.base_model_path, 
                trust_remote_code=True, 
                padding_side='left'
            )
        
        # ç¡®ä¿æœ‰padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model_path}")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        # åŠ è½½å¾®è°ƒæ¨¡å‹
        try:
            logger.info(f"åŠ è½½å¾®è°ƒé€‚é…å™¨: {self.config.model_path}")
            self.model = PeftModel.from_pretrained(base_model, self.config.model_path)
            logger.info("å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½å¾®è°ƒæ¨¡å‹å¤±è´¥: {e}")
            logger.warning("ä½¿ç”¨æœªå¾®è°ƒçš„åŸºç¡€æ¨¡å‹ç»§ç»­")
            self.model = base_model
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
    
    def generate_single_response(self, prompt: str) -> str:
        """ç”Ÿæˆå•ä¸ªå“åº”ï¼Œç”¨äºè¯¦ç»†å±•ç¤º"""
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=self.config.max_length
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                do_sample=False,  # è¯„ä¼°ç”¨è´ªå©ªè§£ç 
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,
                num_beams=1,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if response.startswith(prompt):
            response = response[len(prompt):].strip()
        
        return response
    
    def generate_batch_responses(self, prompts: List[str]) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆæ¨¡å‹å“åº”"""
        all_responses = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(prompts), self.config.batch_size), desc="æ‰¹é‡ç”Ÿæˆ"):
            batch_prompts = prompts[i:i + self.config.batch_size]
            
            # æ‰¹é‡ç¼–ç 
            inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.max_length // 2  # é¢„ç•™ä¸€åŠé•¿åº¦ç»™ç”Ÿæˆ
            ).to(self.model.device)
            
            with torch.no_grad():
                # æ‰¹é‡ç”Ÿæˆ
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.max_new_tokens,
                    temperature=self.config.temperature,
                    do_sample=False,  # è¯„ä¼°ç”¨è´ªå©ªè§£ç 
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                )
            
            # è§£ç å“åº”
            batch_responses = []
            for j, output in enumerate(outputs):
                response = self.tokenizer.decode(output, skip_special_tokens=True)
                # ç§»é™¤è¾“å…¥æç¤ºéƒ¨åˆ†
                original_prompt = batch_prompts[j]
                if response.startswith(original_prompt):
                    response = response[len(original_prompt):].strip()
                batch_responses.append(response)
            
            all_responses.extend(batch_responses)
            
            # æ¸…ç†æ˜¾å­˜
            del inputs, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        return all_responses
    
    def run_batch_test(self):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        logger.info(f"å¼€å§‹æ‰¹é‡æµ‹è¯• (æ ·æœ¬æ•°: {self.config.sample_size}, æ‰¹é‡å¤§å°: {self.config.batch_size})...")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self.load_model_and_tokenizer()
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        
        # è·å–æµ‹è¯•æ ·æœ¬
        test_samples = self.data_handler.get_test_samples(self.config.sample_size)
        logger.info(f"è·å–æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # åˆ›å»ºæç¤º
        prompts = self.data_handler.create_prompts(test_samples)
        
        # å¤„ç†ç»“æœ
        results = []
        details = []
        correct = 0
        format_correct = 0
        
        logger.info("=" * 80)
        logger.info("å‰3ä¸ªæµ‹è¯•æ ·æœ¬çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹:")
        logger.info("=" * 80)

        # å…ˆå¤„ç†å‰3ä¸ªæ ·æœ¬ï¼Œå±•ç¤ºè¯¦ç»†æ¨ç†è¿‡ç¨‹
        for i in range(min(3, len(test_samples))):
            sample = test_samples[i]
            prompt = prompts[i]
            
            logger.info(f"\n{'='*20} æµ‹è¯•æ ·æœ¬ {i+1} {'='*20}")
            logger.info(f"é—®é¢˜: {sample['question']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {sample['answer']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆæ•°å€¼: {self.data_handler.extract_answer_from_text(sample['answer'])}")
            logger.info(f"è¾“å…¥æç¤º:\n{prompt}")
            
            # å•ç‹¬ç”Ÿæˆè¿™ä¸ªæ ·æœ¬çš„å“åº”ï¼Œå±•ç¤ºè¯¦ç»†è¿‡ç¨‹
            response = self.generate_single_response(prompt)
            
            logger.info(f"æ¨¡å‹å®Œæ•´å“åº”:\n{response}")
            
            predicted_answer = self.data_handler.extract_answer_from_text(response)
            ground_truth_answer = self.data_handler.extract_answer_from_text(sample['answer'])
            
            # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
            has_correct_format = self.data_handler.has_correct_format(response)
            if has_correct_format:
                format_correct += 1
            
            # æ¯”è¾ƒç­”æ¡ˆ
            is_correct = (predicted_answer is not None and 
                         ground_truth_answer is not None and 
                         abs(predicted_answer - ground_truth_answer) < 1e-6)
            
            if is_correct:
                correct += 1
            
            logger.info(f"æå–çš„é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
            logger.info(f"æå–çš„æ ‡å‡†ç­”æ¡ˆ: {ground_truth_answer}")
            logger.info(f"ç­”æ¡ˆæ­£ç¡®: {'âœ…' if is_correct else 'âŒ'}")
            logger.info(f"æ ¼å¼æ­£ç¡®: {'âœ…' if has_correct_format else 'âŒ'}")
            
            results.append({'correct': is_correct, 'format_correct': has_correct_format})
            details.append({
                'question': sample['question'],
                'ground_truth': ground_truth_answer,
                'predicted': predicted_answer,
                'correct': is_correct,
                'format_correct': has_correct_format,
                'response': response,
                'prompt': prompt
            })
        
        logger.info("\n" + "=" * 80)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†å‰©ä½™æ ·æœ¬...")
        logger.info("=" * 80)
        
        # æ‰¹é‡å¤„ç†å‰©ä½™æ ·æœ¬
        if len(test_samples) > 3:
            remaining_prompts = prompts[3:]
            remaining_samples = test_samples[3:]
            
            logger.info("æ­£åœ¨æ‰¹é‡ç”Ÿæˆå‰©ä½™å“åº”...")
            remaining_responses = self.generate_batch_responses(remaining_prompts)
            
            logger.info("æ­£åœ¨å¤„ç†å‰©ä½™ç»“æœ...")
            for i, (sample, response) in enumerate(tqdm(zip(remaining_samples, remaining_responses), desc="å¤„ç†å‰©ä½™ç»“æœ")):
                question = sample['question']
                ground_truth_text = sample['answer']
                ground_truth_answer = self.data_handler.extract_answer_from_text(ground_truth_text)
                
                predicted_answer = self.data_handler.extract_answer_from_text(response)
                
                # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
                has_correct_format = self.data_handler.has_correct_format(response)
                if has_correct_format:
                    format_correct += 1
                
                # æ¯”è¾ƒç­”æ¡ˆ
                is_correct = (predicted_answer is not None and 
                             ground_truth_answer is not None and 
                             abs(predicted_answer - ground_truth_answer) < 1e-6)
                
                if is_correct:
                    correct += 1
                
                results.append({'correct': is_correct, 'format_correct': has_correct_format})
                details.append({
                    'question': question,
                    'ground_truth': ground_truth_answer,
                    'predicted': predicted_answer,
                    'correct': is_correct,
                    'format_correct': has_correct_format,
                    'response': response,
                    'prompt': prompts[3 + i]
                })
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        total = len(results)
        accuracy = correct / total if total > 0 else 0
        format_accuracy = format_correct / total if total > 0 else 0
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        logger.info(f"\n" + "=" * 80)
        logger.info("æ‰¹é‡æµ‹è¯•æœ€ç»ˆç»“æœ")
        logger.info("=" * 80)
        logger.info(f"æ€»æ ·æœ¬æ•°: {total}")
        logger.info(f"ç­”æ¡ˆæ­£ç¡®æ•°: {correct}")
        logger.info(f"æ ¼å¼æ­£ç¡®æ•°: {format_correct}")
        logger.info(f"ç­”æ¡ˆå‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.1f}%)")
        logger.info(f"æ ¼å¼å‡†ç¡®ç‡: {format_accuracy:.4f} ({format_accuracy*100:.1f}%)")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = os.path.basename(self.config.model_path.rstrip("/"))
        eval_results = {
            "model_path": self.config.model_path,
            "model_type": self.config.model_type,
            "total_samples": total,
            "correct_predictions": correct,
            "format_correct": format_correct,
            "answer_accuracy": accuracy,
            "format_accuracy": format_accuracy,
            "evaluation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "config": {
                "sample_size": self.config.sample_size,
                "batch_size": self.config.batch_size,
                "temperature": self.config.temperature,
                "max_new_tokens": self.config.max_new_tokens
            },
            "details": details
        }
        
        eval_path = os.path.join(self.config.output_dir, f"{model_name}_{timestamp}_test_results.json")
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_path}")
        
        return eval_results


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡æµ‹è¯•GSM8Kæ•°æ®é›†ä¸Šçš„å¾®è°ƒæ¨¡å‹')
    
    parser.add_argument('--base_model', type=str, default="/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B",
                        help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('--model', type=str, required=True,
                        help='å¾®è°ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--dataset', type=str, default="/root/autodl-tmp/GRPO_MATH/gsm8k",
                        help='GSM8Kæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output', type=str, default="./evaluation_results",
                        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--model_type', type=str, choices=['lora', 'grpo'], default='lora',
                        help='æ¨¡å‹ç±»å‹ (lora æˆ– grpo)')
    parser.add_argument('--sample_size', type=int, default=100,
                        help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--temperature', type=float, default=0.1,
                        help='ç”Ÿæˆæ¸©åº¦')
    parser.add_argument('--max_new_tokens', type=int, default=256,
                        help='æœ€å¤§ç”Ÿæˆtokenæ•°é‡')
    parser.add_argument('--no_bf16', action='store_true',
                        help='ä¸ä½¿ç”¨bf16')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        base_model_path=args.base_model,
        model_path=args.model,
        dataset_path=args.dataset,
        output_dir=args.output,
        model_type=args.model_type,
        sample_size=args.sample_size,
        batch_size=args.batch_size,
        temperature=args.temperature,
        max_new_tokens=args.max_new_tokens,
        use_bf16=not args.no_bf16,
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.output_dir, exist_ok=True)
    
    # è®°å½•é…ç½®
    config_path = os.path.join(config.output_dir, f"test_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        # è½¬æ¢ä¸ºå­—å…¸å¹¶ä¿å­˜
        config_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('_')}
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
        tester = ModelTester(config)
        
        logger.info("=" * 50)
        logger.info(f"å¼€å§‹å¯¹æ¨¡å‹ {config.model_path} è¿›è¡Œæ‰¹é‡æµ‹è¯•")
        logger.info("=" * 50)
        
        # è¿è¡Œæµ‹è¯•
        eval_results = tester.run_batch_test()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        logger.info("=" * 50)
        logger.info("æ‰¹é‡æµ‹è¯•å®Œæˆï¼æœ€ç»ˆç»“æœæ‘˜è¦ï¼š")
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        logger.info(f"   - æµ‹è¯•æ ·æœ¬æ•°: {eval_results['total_samples']}")
        logger.info(f"   - ç­”æ¡ˆæ­£ç¡®æ•°: {eval_results['correct_predictions']}")
        logger.info(f"   - ç­”æ¡ˆå‡†ç¡®ç‡: {eval_results['answer_accuracy']:.4f} ({eval_results['answer_accuracy']*100:.1f}%)")
        logger.info(f"   - æ ¼å¼æ­£ç¡®æ•°: {eval_results['format_correct']}")
        logger.info(f"   - æ ¼å¼å‡†ç¡®ç‡: {eval_results['format_accuracy']:.4f} ({eval_results['format_accuracy']*100:.1f}%)")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†CUDAç¼“å­˜")


if __name__ == "__main__":
    main()
