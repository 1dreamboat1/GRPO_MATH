import os
import re
import json
import torch
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Union
import logging
from datetime import datetime
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer, 
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, PeftModel
from datasets import load_dataset, Dataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # è·¯å¾„é…ç½®
    model_path: str = "/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B"
    dataset_path: str = "/root/autodl-tmp/GRPO_MATH/gsm8k"
    output_dir: str = "./lora_finetuned_qwen"
    
    # è®­ç»ƒå‚æ•°
    max_length: int = 256
    batch_size: int = 2
    gradient_accumulation_steps: int = 8
    num_epochs: int = 2
    learning_rate: float = 2e-5
    warmup_steps: int = 150
    weight_decay: float = 0.01
    train_val_split_ratio: float = 0.9
    
    # LoRAå‚æ•°
    lora_rank: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    target_modules: List[str] = None
    
    # å…¶ä»–é…ç½®
    use_bf16: bool = True
    logging_steps: int = 10
    
    def __post_init__(self):
        if self.target_modules is None:
            # æ‰©å±•LoRAåº”ç”¨çš„æ¨¡å—ï¼Œå¢åŠ æ›´å¤šå¯è®­ç»ƒå±‚
            self.target_modules = ["q_proj", "k_proj", "v_proj"]

class GSM8KDataHandler:
    """GSM8Kæ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, config: TrainingConfig, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.train_data = None
        self.test_data = None
    
    def extract_answer_from_text(self, answer_text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å¯»æ‰¾ #### åé¢çš„æ•°å­—
        pattern = r'####\s*(-?\d+(?:\.\d+)?)'
        match = re.search(pattern, answer_text)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°####ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        
        return None
        
    def load_dataset(self) -> Tuple[Dataset, Dataset]:
        """åŠ è½½GSM8Kæ•°æ®é›†"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæ•°æ®é›†...")
        
        try:
            # åŠ è½½Parquetæ ¼å¼
            train_parquet_path = os.path.join(self.config.dataset_path, "main/train-00000-of-00001.parquet")
            test_parquet_path = os.path.join(self.config.dataset_path, "main/test-00000-of-00001.parquet")
            
            if os.path.exists(train_parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½æ•°æ®é›†")
                dataset = load_dataset("parquet", data_files={
                    "train": train_parquet_path,
                    "test": test_parquet_path
                })
                self.train_data = list(dataset['train'])
                self.test_data = list(dataset['test'])
            
                
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(self.train_data)}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(self.test_data)}")
        
        # æ•°æ®é¢„å¤„ç† - è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼
        processed_train_data = self._preprocess_data_with_conversation(self.train_data)
        
        # åˆ›å»ºDatasetå¯¹è±¡å¹¶åˆ’åˆ†
        full_dataset = Dataset.from_list(processed_train_data)
        dataset_split = full_dataset.train_test_split(
            test_size=1 - self.config.train_val_split_ratio, 
            seed=42
        )
        
        train_dataset = dataset_split['train']
        val_dataset = dataset_split['test']
        
        logger.info(f"åˆ’åˆ†åè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def generate_conversation(self, examples):
        """å°†æ•°æ®è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼"""
        questions = examples["question"] if isinstance(examples["question"], list) else [examples["question"]]
        answers = examples["answer"] if isinstance(examples["answer"], list) else [examples["answer"]]
        
        conversations = []
        for question, answer in zip(questions, answers):
            conversations.append([
                {"role": "user", "content": "Please solve this math problem step by step and provide your final answer after the \"####\" marker.\n"+question},
                {"role": "assistant", "content": answer},
            ])
        
        return {"conversations": conversations}
    
    def get_test_samples(self, n_samples: Optional[int] = None) -> List[Dict]:
        """è·å–æµ‹è¯•æ ·æœ¬"""
        if self.test_data is None:
            raise ValueError("æµ‹è¯•æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_dataset()")
            
        if n_samples is None:
            return self.test_data
        return self.test_data[:min(n_samples, len(self.test_data))]
    
    def _preprocess_data_with_conversation(self, raw_data: List[Dict]) -> List[Dict]:
        """é¢„å¤„ç†åŸå§‹æ•°æ®ï¼Œè½¬æ¢ä¸ºå¯¹è¯æ ¼å¼"""
        processed_data = []
        
        # åˆ›å»ºä¸´æ—¶Datasetç”¨äºæ‰¹å¤„ç†
        temp_dataset = Dataset.from_list(raw_data)
        
        # åº”ç”¨å¯¹è¯è½¬æ¢
        conversations_dataset = temp_dataset.map(
            self.generate_conversation, 
            batched=True,
            desc="Converting to conversation format"
        )
        
        # åº”ç”¨chat template
        logger.info("åº”ç”¨chat template...")
        formatted_conversations = []
        
        for conversations in tqdm(conversations_dataset["conversations"], desc="Applying chat template"):
            try:
                # åº”ç”¨chat template
                formatted_text = self.tokenizer.apply_chat_template(
                    conversations,
                    tokenize=False,  # ä¸è¿›è¡Œåˆ†è¯ï¼Œä»…åº”ç”¨æ¨¡æ¿
                    add_generation_prompt=True  # æ·»åŠ ç”Ÿæˆæç¤º
                )
                formatted_conversations.append(formatted_text)
            except Exception as e:
                logger.warning(f"åº”ç”¨chat templateå¤±è´¥: {e}")
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼
                question = conversations[0]["content"]
                answer = conversations[1]["content"]
                formatted_text = f"é—®é¢˜: {question}\nå›ç­”: {answer}"
                formatted_conversations.append(formatted_text)
        
        # æ„å»ºæœ€ç»ˆæ•°æ®
        for i, (sample, formatted_text) in enumerate(zip(raw_data, formatted_conversations)):
            # æå–æœ€ç»ˆç­”æ¡ˆæ•°å­—
            answer_label = self.extract_answer_from_text(sample['answer'])
            
            # æ£€æŸ¥ç­”æ¡ˆæå–æ˜¯å¦æˆåŠŸ
            if answer_label is None:
                logger.warning(f"æ ·æœ¬ {i} ç­”æ¡ˆæ•°å­—æå–å¤±è´¥: {sample['answer'][:100]}...")

            processed_data.append({
                "text": formatted_text,
                "answer": sample['answer'],
                "answer_label": answer_label,
                "question": sample['question']
            })
        
        logger.info(f"å¯¹è¯æ ¼å¼é¢„å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°é‡: {len(processed_data)}")
        
        # å±•ç¤ºå‰3ä¸ªæ ·æœ¬
        logger.info("å¯¹è¯æ ¼å¼æ ·æœ¬ç¤ºä¾‹:")
        for i in range(min(3, len(processed_data))):
            logger.info(f"\n=== æ ·æœ¬ {i+1} ===")
            logger.info(f"æ ¼å¼åŒ–æ–‡æœ¬:\n{processed_data[i]['text']}")
            logger.info(f"é—®é¢˜: {processed_data[i]['question']}")
            logger.info(f"ç­”æ¡ˆ: {processed_data[i]['answer']}")
            logger.info(f"ç­”æ¡ˆæ ‡ç­¾: {processed_data[i]['answer_label']}")
            
        return processed_data
    
    def tokenize_dataset(self, dataset: Dataset) -> Dataset:
        """å¯¹æ•°æ®é›†è¿›è¡Œtokenizationï¼Œåªå¯¹assistantå›ç­”éƒ¨åˆ†è®¡ç®—loss"""
        logger.info("å¼€å§‹tokenization...")
        
        def tokenize_function(examples):
            """å¯¹æ•°æ®é›†è¿›è¡Œtokenizationï¼Œåªå¯¹assistantå›ç­”éƒ¨åˆ†è®¡ç®—loss"""
            # å­˜å‚¨æ‰¹æ¬¡ç»“æœ
            batch_input_ids = []
            batch_attention_mask = []
            batch_labels = []
    
            texts = examples["text"] if isinstance(examples["text"], list) else [examples["text"]]
    
            for text in texts:
                # é¦–å…ˆç¼–ç å®Œæ•´æ–‡æœ¬
                full_encoding = self.tokenizer(
                    text,
                    padding=False,
                    truncation=True,
                    max_length=self.config.max_length,  # ç¡®ä¿æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
                    return_tensors=None
                )
        
                input_ids = full_encoding["input_ids"]
                attention_mask = full_encoding["attention_mask"]
        
                # åˆ›å»ºlabelsï¼Œåˆå§‹åŒ–ä¸º-100ï¼ˆå¿½ç•¥lossè®¡ç®—ï¼‰
                labels = [-100] * len(input_ids)
        
                # æŸ¥æ‰¾assistantå›ç­”çš„å¼€å§‹ä½ç½®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                try:
                    text_decoded = self.tokenizer.decode(input_ids, skip_special_tokens=False)
                    # logger.info(f"è§£ç åçš„æ–‡æœ¬: {text_decoded}")  
            
                    assistant_markers = [
                        "<|im_start|>assistant\n",
                        "assistant\n", 
                        "å›ç­”:",
                        "Assistant:",
                        "<|assistant|>",
                    ]
            
                    assistant_start_idx = None
                    for marker in assistant_markers:
                        if marker in text:
                            marker_pos = text.find(marker)
                            if marker_pos != -1:
                                content_start = marker_pos + len(marker)
                                prefix_text = text[:content_start]
                                prefix_tokens = self.tokenizer(
                                    prefix_text,
                                    padding=False,
                                    truncation=False,
                                    return_tensors=None
                                )["input_ids"]
                        
                                assistant_start_idx = len(prefix_tokens)
                                break
            
                    if assistant_start_idx is not None and assistant_start_idx < len(labels):
                        for i in range(assistant_start_idx, len(labels)):
                            labels[i] = input_ids[i]
                    else:
                        mid_point = len(labels) // 2
                        for i in range(mid_point, len(labels)):
                            labels[i] = input_ids[i]
        
                except Exception as e:
                    logger.warning(f"å®šä½assistantå›ç­”å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {e}")
                    mid_point = len(labels) // 2
                    for i in range(mid_point, len(labels)):
                        labels[i] = input_ids[i]
        
                batch_input_ids.append(input_ids)
                batch_attention_mask.append(attention_mask)
                batch_labels.append(labels)

            # logger.info(f"ç”Ÿæˆçš„batch_labels: {batch_labels[:2]}")
            # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶paddingåˆ°max_length
            target_length = self.config.max_length
    
            for i in range(len(batch_input_ids)):
                current_length = len(batch_input_ids[i])
        
                if current_length < target_length:
                    # Paddingåˆ°ç›®æ ‡é•¿åº¦
                    pad_length = target_length - current_length
                    batch_input_ids[i] = batch_input_ids[i] + [self.tokenizer.pad_token_id] * pad_length
                    batch_attention_mask[i] = batch_attention_mask[i] + [0] * pad_length
                    batch_labels[i] = batch_labels[i] + [-100] * pad_length
                elif current_length > target_length:
                    # æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                    batch_input_ids[i] = batch_input_ids[i][:target_length]
                    batch_attention_mask[i] = batch_attention_mask[i][:target_length]
                    batch_labels[i] = batch_labels[i][:target_length]
    
            return {
                "input_ids": batch_input_ids,
                "attention_mask": batch_attention_mask,
                "labels": batch_labels
            }
        # åº”ç”¨tokenization
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True, 
            num_proc=2,
            desc="Tokenizing dataset with proper loss masking"
        )
        
        # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—å¹¶è®¾ç½®æ ¼å¼
        tokenized_dataset = tokenized_dataset.remove_columns([col for col in dataset.column_names if col not in ["input_ids", "attention_mask", "labels"]])
        tokenized_dataset.set_format("torch")
        
        # éªŒè¯tokenizationç»“æœ
        logger.info("éªŒè¯tokenizationç»“æœ...")
        sample = tokenized_dataset[0]
        labels = sample["labels"]
        
        # ç»Ÿè®¡æœ‰æ•ˆlabelçš„æ•°é‡ï¼ˆä¸æ˜¯-100çš„ï¼‰
        valid_labels = sum(1 for label in labels if label != -100)
        total_labels = len(labels)
        
        logger.info(f"æ ·æœ¬æ ‡ç­¾ç»Ÿè®¡: æœ‰æ•ˆæ ‡ç­¾ {valid_labels}/{total_labels} ({100*valid_labels/total_labels:.1f}%)")
        
        # æ˜¾ç¤ºä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        input_ids = sample["input_ids"]
        logger.info("tokenizationæ ·æœ¬æ£€æŸ¥:")
        logger.info(f"åŸæ ·ä¾‹æ–‡æœ¬ : {self.tokenizer.decode(input_ids, skip_special_tokens=True)}")
        logger.info(f"è¾“å…¥åºåˆ—é•¿åº¦: {len(input_ids)}")
        logger.info(f"æ ‡ç­¾åºåˆ—é•¿åº¦: {len(labels)}")
        
        # æ˜¾ç¤ºå“ªäº›éƒ¨åˆ†ä¼šå‚ä¸lossè®¡ç®—
        valid_positions = [i for i, label in enumerate(labels) if label != -100]
        if valid_positions:
            logger.info(f"å‚ä¸lossè®¡ç®—çš„ä½ç½®èŒƒå›´: {valid_positions[0]} åˆ° {valid_positions[-1]}")
            
            # è§£ç å‚ä¸lossè®¡ç®—çš„éƒ¨åˆ†
            valid_tokens = [input_ids[i] for i in valid_positions]
            decoded_valid = self.tokenizer.decode(valid_tokens, skip_special_tokens=True)
            logger.info(f"å‚ä¸lossè®¡ç®—çš„æ–‡æœ¬: {decoded_valid}")
        
        return tokenized_dataset

class QwenLoRATrainer:
    """Qwen LoRAå¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self._setup_environment()
        self.tokenizer = None
        self.model = None
        self.trainer = None
        self.data_handler = None
        
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: {self.config.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {base_model.num_parameters():,}")
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.target_modules,
            task_type="CAUSAL_LM"
        )
        
        self.model = get_peft_model(base_model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
    def prepare_datasets(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        
        # åŠ è½½åŸå§‹æ•°æ®é›†
        train_dataset, val_dataset = self.data_handler.load_dataset()
        
        # tokenization
        train_dataset = self.data_handler.tokenize_dataset(train_dataset)
        val_dataset = self.data_handler.tokenize_dataset(val_dataset)
        
        return train_dataset, val_dataset
        
    def setup_trainer(self, train_dataset: Dataset, val_dataset: Dataset):
        """è®¾ç½®è®­ç»ƒå™¨"""
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            weight_decay=self.config.weight_decay,
            logging_dir=os.path.join(self.config.output_dir, "logs"),
            logging_steps=self.config.logging_steps,
            # evaluation_strategy="epoch",
            evaluation_strategy="steps",
            # save_strategy="epoch",
            save_strategy="steps",
            eval_steps=100,  # æ¯200æ­¥è¯„ä¼°ä¸€æ¬¡
            save_steps=100,  # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=not self.config.use_bf16,
            bf16=self.config.use_bf16,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            max_grad_norm=0.5,
            optim="adamw_torch",  # ä½¿ç”¨ç¨³å®šä¼˜åŒ–å™¨
        )
        
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
        )
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†"""
        logger.info("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†...")
        
        # å¦‚æœå·²ç»æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨å®ƒ
        if self.model is not None:
            logger.info("ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
        
        # å¦åˆ™ä»ä¿å­˜çš„è·¯å¾„åŠ è½½
        if os.path.exists(self.config.output_dir):
            logger.info(f"ä» {self.config.output_dir} åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
            
            # é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            base_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            self.model = PeftModel.from_pretrained(base_model, self.config.output_dir)
            logger.info("å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            raise ValueError(f"æ‰¾ä¸åˆ°å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„: {self.config.output_dir}")
        
    def train(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. å‡†å¤‡æ•°æ®é›†
            train_dataset, val_dataset = self.prepare_datasets()
            
            # 3. è®¾ç½®è®­ç»ƒå™¨
            self.setup_trainer(train_dataset, val_dataset)
            
            # 4. æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # 5. å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒ...")
            self.trainer.train()
            
            # 6. ä¿å­˜æ¨¡å‹
            logger.info("ä¿å­˜æ¨¡å‹...")
            self.model.save_pretrained(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            logger.info("è®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def generate_single_response(self, prompt: str, max_new_tokens: int = 256, temperature: float = 0.1) -> str:
        """ç”Ÿæˆå•ä¸ªå“åº”ï¼Œç”¨äºè¯¦ç»†å±•ç¤º"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                # do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,
                num_beams=1,
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        if response.startswith(prompt):
            response = response[len(prompt):].strip()
        
        return response
    
    def generate_batch_responses(self, prompts: List[str], max_new_tokens: int = 256, 
                               temperature: float = 0.1, batch_size: int = 8) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆæ¨¡å‹å“åº” - æ ¸å¿ƒä¼˜åŒ–ç‚¹
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆæ–°tokenæ•°é‡
            temperature: æ¸©åº¦å‚æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        all_responses = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(prompts), batch_size), desc="æ‰¹é‡ç”Ÿæˆ"):
            batch_prompts = prompts[i:i + batch_size]
            
            # æ‰¹é‡ç¼–ç 
            inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=256  # é™åˆ¶è¾“å…¥é•¿åº¦é˜²æ­¢æ˜¾å­˜ä¸è¶³
            ).to(self.model.device)
            
            with torch.no_grad():
                # æ‰¹é‡ç”Ÿæˆ
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    # do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    # å…³é”®ä¼˜åŒ–å‚æ•°
                    use_cache=True,  # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
                    num_beams=1,     
                )
            
            # è§£ç å“åº”
            batch_responses = []
            for j, output in enumerate(outputs):
                response = self.tokenizer.decode(output, skip_special_tokens=True)
                # ç§»é™¤è¾“å…¥æç¤ºéƒ¨åˆ†
                original_prompt = batch_prompts[j]
                if response.startswith(original_prompt):
                    response = response[len(original_prompt):].strip()
                batch_responses.append(response)
            
            all_responses.extend(batch_responses)
            
            # æ¸…ç†æ˜¾å­˜
            del inputs, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        return all_responses
            
    def evaluate_on_test_set(self, sample_size: int = 100, batch_size: int = 8):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå±•ç¤ºå‰2ä¸ªè¯¦ç»†ç»“æœ"""
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼°æµ‹è¯•é›† (æ ·æœ¬æ•°: {sample_size}, æ‰¹é‡å¤§å°: {batch_size})...")
        
        # ç¡®ä¿ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
        self.load_finetuned_model()
        
        # ç¡®ä¿æ•°æ®å¤„ç†å™¨å·²åˆå§‹åŒ–
        if self.data_handler is None:
            self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
            # å¦‚æœéœ€è¦ï¼Œé‡æ–°åŠ è½½æ•°æ®
            if self.data_handler.test_data is None:
                self.data_handler.load_dataset()
        
        # è·å–æµ‹è¯•æ ·æœ¬
        test_samples = self.data_handler.get_test_samples(sample_size)
        logger.info(f"è·å–æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # åˆ›å»ºæç¤ºï¼ˆä½¿ç”¨å¯¹è¯æ ¼å¼ï¼‰
        prompts = []
        for sample in test_samples:
            # ä½¿ç”¨chat templateæ ¼å¼åŒ–æµ‹è¯•é—®é¢˜
            try:
                conversation = [
                    {"role": "user", "content": "Please solve this math problem step by step and provide your final answer after the \"####\" marker.\n"+sample['question']},
                ]
                prompt = self.tokenizer.apply_chat_template(
                    conversation,
                    tokenize=False,
                    add_generation_prompt=True  #æ·»åŠ ç”Ÿæˆæç¤º
                )
            except:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                prompt = f"Please solve this math problem step by step and provide your final answer after the \"####\" marker.\nquestion: {sample['question']}\nanswer: "
            prompts.append(prompt)
        
        self.model.eval()
        
        # å¤„ç†ç»“æœ
        results = []
        details = []
        correct = 0
        
        logger.info("=" * 80)
        logger.info("å‰10ä¸ªæµ‹è¯•æ ·æœ¬çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹:")
        logger.info("=" * 80)

        # å…ˆå¤„ç†å‰2ä¸ªæ ·æœ¬ï¼Œå±•ç¤ºè¯¦ç»†æ¨ç†è¿‡ç¨‹
        for i in range(min(2, len(test_samples))):
            sample = test_samples[i]
            prompt = prompts[i]
            
            logger.info(f"\n{'='*20} æµ‹è¯•æ ·æœ¬ {i+1} {'='*20}")
            logger.info(f"é—®é¢˜: {sample['question']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {sample['answer']}")
            logger.info(f"æ ‡å‡†ç­”æ¡ˆæ•°å€¼: {self.data_handler.extract_answer_from_text(sample['answer'])}")
            logger.info(f"è¾“å…¥æç¤º:\n{prompt}")
            
            # å•ç‹¬ç”Ÿæˆè¿™ä¸ªæ ·æœ¬çš„å“åº”ï¼Œå±•ç¤ºè¯¦ç»†è¿‡ç¨‹
            response = self.generate_single_response(prompt, max_new_tokens=256)
            
            logger.info(f"æ¨¡å‹å®Œæ•´å“åº”:\n{response}")
            
            predicted_answer = self.data_handler.extract_answer_from_text(response)
            ground_truth_answer = self.data_handler.extract_answer_from_text(sample['answer'])
            
            # æ¯”è¾ƒç­”æ¡ˆ
            is_correct = (predicted_answer is not None and 
                         ground_truth_answer is not None and 
                         abs(predicted_answer - ground_truth_answer) < 1e-6)
            
            if is_correct:
                correct += 1
            
            logger.info(f"æå–çš„é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
            logger.info(f"æå–çš„æ ‡å‡†ç­”æ¡ˆ: {ground_truth_answer}")
            logger.info(f"æ˜¯å¦æ­£ç¡®: {'âœ… æ­£ç¡®' if is_correct else 'âŒ é”™è¯¯'}")
            
            results.append({'correct': is_correct})
            details.append({
                'question': sample['question'],
                'ground_truth': ground_truth_answer,
                'predicted': predicted_answer,
                'correct': is_correct,
                'response': response,
                'prompt': prompt
            })
        
        logger.info("\n" + "=" * 80)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†å‰©ä½™æ ·æœ¬...")
        logger.info("=" * 80)
        
        # æ‰¹é‡å¤„ç†å‰©ä½™æ ·æœ¬
        if len(test_samples) > 2:
            remaining_prompts = prompts[2:]
            remaining_samples = test_samples[2:]
            
            logger.info("æ­£åœ¨æ‰¹é‡ç”Ÿæˆå‰©ä½™å“åº”...")
            remaining_responses = self.generate_batch_responses(remaining_prompts, batch_size=batch_size)
            
            logger.info("æ­£åœ¨å¤„ç†å‰©ä½™ç»“æœ...")
            for i, (sample, response) in enumerate(tqdm(zip(remaining_samples, remaining_responses), desc="å¤„ç†å‰©ä½™ç»“æœ")):
                question = sample['question']
                ground_truth_text = sample['answer']
                ground_truth_answer = self.data_handler.extract_answer_from_text(ground_truth_text)
                
                predicted_answer = self.data_handler.extract_answer_from_text(response)
                
                # æ¯”è¾ƒç­”æ¡ˆ
                is_correct = (predicted_answer is not None and 
                             ground_truth_answer is not None and 
                             abs(predicted_answer - ground_truth_answer) < 1e-6)
                
                if is_correct:
                    correct += 1
                
                results.append({'correct': is_correct})
                details.append({
                    'question': question,
                    'ground_truth': ground_truth_answer,
                    'predicted': predicted_answer,
                    'correct': is_correct,
                    'response': response,
                    'prompt': prompts[2 + i]
                })
        
        total = len(results)
        accuracy = correct / total if total > 0 else 0
        
        logger.info(f"\n" + "=" * 80)
        logger.info("æœ€ç»ˆè¯„ä¼°ç»“æœ")
        logger.info("=" * 80)
        logger.info(f"æ€»æ ·æœ¬æ•°: {total}")
        logger.info(f"æ­£ç¡®é¢„æµ‹: {correct}")
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_results = {
            "total_samples": total,
            "correct_predictions": correct,
            "accuracy": accuracy,
            "evaluation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "batch_size": batch_size,
            "model_config": {
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "learning_rate": self.config.learning_rate,
                "num_epochs": self.config.num_epochs
            },
            "details": details
        }
        
        eval_path = os.path.join(self.config.output_dir, "detailed_test_evaluation_results.json")
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_path}")
        
        return eval_results

def save_config(config: TrainingConfig, filepath: str):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config_dict = {
        'model_path': config.model_path,
        'dataset_path': config.dataset_path,
        'output_dir': config.output_dir,
        'max_length': config.max_length,
        'batch_size': config.batch_size,
        'gradient_accumulation_steps': config.gradient_accumulation_steps,
        'num_epochs': config.num_epochs,
        'learning_rate': config.learning_rate,
        'lora_rank': config.lora_rank,
        'lora_alpha': config.lora_alpha,
        'lora_dropout': config.lora_dropout,
        'target_modules': config.target_modules,
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

def load_config(filepath: str) -> TrainingConfig:
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    return TrainingConfig(**config_dict)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = TrainingConfig(
        # model_path="/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B",
        # dataset_path="/root/autodl-tmp/GRPO_MATH/gsm8k",
        # output_dir="./lora_finetuned_qwen",
        # max_length=512,
        # batch_size=8,
        # gradient_accumulation_steps=2,
        # num_epochs=3,
        # lora_rank=8,
        # lora_alpha=32,
        # lora_dropout=0.1
    )
    
    # ä¿å­˜é…ç½®
    os.makedirs(config.output_dir, exist_ok=True)
    save_config(config, os.path.join(config.output_dir, "training_config.json"))
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡Œè®­ç»ƒ
        trainer = QwenLoRATrainer(config)
        
        # æ‰§è¡Œè®­ç»ƒ
        logger.info("=" * 50)
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        logger.info("=" * 50)
        trainer.train()
        
        # è®­ç»ƒå®Œæˆåè¿›è¡Œæ‰¹é‡æµ‹è¯•é›†è¯„ä¼°
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æ‰¹é‡æµ‹è¯•é›†è¯„ä¼°")
        logger.info("=" * 50)
        eval_results = trainer.evaluate_on_test_set(sample_size=1300, batch_size=32)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼æœ€ç»ˆç»“æœæ‘˜è¦ï¼š")
        logger.info("=" * 50)
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è·¯å¾„: {config.output_dir}")
        logger.info(f"ğŸ“Š æ‰¹é‡æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        logger.info(f"   - æµ‹è¯•æ ·æœ¬æ•°: {eval_results['total_samples']}")
        logger.info(f"   - æ­£ç¡®é¢„æµ‹æ•°: {eval_results['correct_predictions']}")
        logger.info(f"   - å‡†ç¡®ç‡: {eval_results['accuracy']:.3f} ({eval_results['accuracy']*100:.1f}%)")
        logger.info(f"ğŸ“„ è¯¦ç»†è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {os.path.join(config.output_dir, 'batch_test_evaluation_results.json')}")
        
        logger.info("=" * 50)
        logger.info("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_info = {
            "error_message": str(e),
            "error_type": type(e).__name__,
            "config": {
                "model_path": config.model_path,
                "dataset_path": config.dataset_path,
                "output_dir": config.output_dir,
                "batch_size": config.batch_size,
                "num_epochs": config.num_epochs,
                "lora_rank": config.lora_rank,
                "lora_alpha": config.lora_alpha,
                "lora_dropout": config.lora_dropout
            }
        }

        error_path = os.path.join(config.output_dir, "error_log.json")
        with open(error_path, 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {error_path}")
    
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†CUDAç¼“å­˜")

if __name__ == "__main__":
    main()