import os
import re
import json
import torch
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Union
import logging
from datetime import datetime
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer, 
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, PeftModel
from datasets import load_dataset, Dataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # è·¯å¾„é…ç½®
    model_path: str = "/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B"
    dataset_path: str = "/root/autodl-tmp/GRPO_MATH/gsm8k"
    output_dir: str = "./lora_finetuned_qwen"
    
    # è®­ç»ƒå‚æ•°
    max_length: int = 512
    batch_size: int = 4
    gradient_accumulation_steps: int = 2
    num_epochs: int = 3
    learning_rate: float = 2e-4
    warmup_steps: int = 100
    weight_decay: float = 0.01
    train_val_split_ratio: float = 0.8
    
    # LoRAå‚æ•°
    lora_rank: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: List[str] = None
    
    # å…¶ä»–é…ç½®
    use_bf16: bool = True
    logging_steps: int = 10
    
    def __post_init__(self):
        if self.target_modules is None:
            self.target_modules = ["q_proj", "v_proj"]

class GSM8KDataHandler:
    """GSM8Kæ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, config: TrainingConfig, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.train_data = None
        self.test_data = None
    
    def extract_answer_from_text(self, answer_text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å¯»æ‰¾ #### åé¢çš„æ•°å­—
        pattern = r'####\s*(-?\d+(?:\.\d+)?)'
        match = re.search(pattern, answer_text)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°####ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        
        return None
        
    def load_dataset(self) -> Tuple[Dataset, Dataset]:
        """åŠ è½½GSM8Kæ•°æ®é›†"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæ•°æ®é›†...")
        
        try:
            # åŠ è½½Parquetæ ¼å¼
            train_parquet_path = os.path.join(self.config.dataset_path, "main/train-00000-of-00001.parquet")
            test_parquet_path = os.path.join(self.config.dataset_path, "main/test-00000-of-00001.parquet")
            
            if os.path.exists(train_parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½æ•°æ®é›†")
                dataset = load_dataset("parquet", data_files={
                    "train": train_parquet_path,
                    "test": test_parquet_path
                })
                self.train_data = list(dataset['train'])
                self.test_data = list(dataset['test'])
            else:
                logger.info("å°è¯•ä»HuggingFaceåŠ è½½GSM8K...")
                dataset = load_dataset("gsm8k", "default")
                self.train_data = list(dataset['train'])
                self.test_data = list(dataset['test'])
                
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(self.train_data)}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(self.test_data)}")
        
        # æ•°æ®é¢„å¤„ç†
        processed_train_data = self._preprocess_data(self.train_data)
        
        # åˆ›å»ºDatasetå¯¹è±¡å¹¶åˆ’åˆ†
        full_dataset = Dataset.from_list(processed_train_data)
        dataset_split = full_dataset.train_test_split(
            test_size=1 - self.config.train_val_split_ratio, 
            seed=42
        )
        
        train_dataset = dataset_split['train']
        val_dataset = dataset_split['test']
        
        logger.info(f"åˆ’åˆ†åè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def get_test_samples(self, n_samples: Optional[int] = None) -> List[Dict]:
        """è·å–æµ‹è¯•æ ·æœ¬"""
        if self.test_data is None:
            raise ValueError("æµ‹è¯•æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_dataset()")
            
        if n_samples is None:
            return self.test_data
        return self.test_data[:min(n_samples, len(self.test_data))]
    
    def _preprocess_data(self, raw_data: List[Dict]) -> List[Dict]:
        """é¢„å¤„ç†åŸå§‹æ•°æ®ï¼Œæå–ç­”æ¡ˆæ ‡ç­¾"""
        processed_data = []
        
        for sample in raw_data:
            # æå–æœ€ç»ˆç­”æ¡ˆæ•°å­—
            answer_label = self.extract_answer_from_text(sample['answer'])
            
            # æ„é€ è¾“å…¥æ–‡æœ¬ï¼Œä¿æŒCoTæ ¼å¼
            input_text = f"é—®é¢˜: {sample['question']}\nå›ç­”: {sample['answer']}"
            
            processed_data.append({
                "text": input_text,
                "answer": sample['answer'],
                "answer_label": answer_label,
                "question": sample['question']
            })
        
        logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°é‡: {len(processed_data)}")
        return processed_data
    
    def tokenize_dataset(self, dataset: Dataset) -> Dataset:
        """å¯¹æ•°æ®é›†è¿›è¡Œtokenization"""
        logger.info("å¼€å§‹tokenization...")
        
        def tokenize_function(examples):
            # ç¼–ç å®Œæ•´æ–‡æœ¬ï¼ˆé—®é¢˜+å›ç­”ï¼‰
            encodings = self.tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=self.config.max_length,
                return_tensors="pt"
            )
            
            # ç¼–ç ç­”æ¡ˆéƒ¨åˆ†ä½œä¸ºlabels
            answer_encodings = self.tokenizer(
                examples["answer"],
                padding="max_length",
                truncation=True,
                max_length=self.config.max_length,
                return_tensors="pt"
            )
            
            # è®¾ç½®labelsï¼Œå¿½ç•¥paddingéƒ¨åˆ†
            encodings["labels"] = answer_encodings["input_ids"].clone()
            encodings["labels"][answer_encodings["attention_mask"] == 0] = -100
            
            return encodings
        
        # åº”ç”¨tokenization
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True, 
            num_proc=1,
            desc="Tokenizing dataset"
        )
        
        # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—å¹¶è®¾ç½®æ ¼å¼
        tokenized_dataset = tokenized_dataset.remove_columns([col for col in dataset.column_names if col not in ["input_ids", "attention_mask", "labels"]])
        tokenized_dataset.set_format("torch")
        
        return tokenized_dataset

class QwenLoRATrainer:
    """Qwen LoRAå¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self._setup_environment()
        self.tokenizer = None
        self.model = None
        self.trainer = None
        self.data_handler = None
        
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: {self.config.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {base_model.num_parameters():,}")
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.target_modules,
            task_type="CAUSAL_LM"
        )
        
        self.model = get_peft_model(base_model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
    def prepare_datasets(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        
        # åŠ è½½åŸå§‹æ•°æ®é›†
        train_dataset, val_dataset = self.data_handler.load_dataset()
        
        # tokenization
        train_dataset = self.data_handler.tokenize_dataset(train_dataset)
        val_dataset = self.data_handler.tokenize_dataset(val_dataset)
        
        return train_dataset, val_dataset
        
    def setup_trainer(self, train_dataset: Dataset, val_dataset: Dataset):
        """è®¾ç½®è®­ç»ƒå™¨"""
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            weight_decay=self.config.weight_decay,
            logging_dir=os.path.join(self.config.output_dir, "logs"),
            logging_steps=self.config.logging_steps,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=not self.config.use_bf16,
            bf16=self.config.use_bf16,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
        )
        
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
        )
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†"""
        logger.info("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†...")
        
        # å¦‚æœå·²ç»æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨å®ƒ
        if self.model is not None:
            logger.info("ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
        
        # å¦åˆ™ä»ä¿å­˜çš„è·¯å¾„åŠ è½½
        if os.path.exists(self.config.output_dir):
            logger.info(f"ä» {self.config.output_dir} åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
            
            # é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            base_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            self.model = PeftModel.from_pretrained(base_model, self.config.output_dir)
            logger.info("å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            raise ValueError(f"æ‰¾ä¸åˆ°å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„: {self.config.output_dir}")
        
    def train(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. å‡†å¤‡æ•°æ®é›†
            train_dataset, val_dataset = self.prepare_datasets()
            
            # 3. è®¾ç½®è®­ç»ƒå™¨
            self.setup_trainer(train_dataset, val_dataset)
            
            # 4. æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # 5. å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒ...")
            self.trainer.train()
            
            # 6. ä¿å­˜æ¨¡å‹
            logger.info("ä¿å­˜æ¨¡å‹...")
            self.model.save_pretrained(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            logger.info("è®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def generate_batch_responses(self, prompts: List[str], max_new_tokens: int = 256, 
                               temperature: float = 0.1, batch_size: int = 8) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆæ¨¡å‹å“åº” - æ ¸å¿ƒä¼˜åŒ–ç‚¹
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆæ–°tokenæ•°é‡
            temperature: æ¸©åº¦å‚æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        all_responses = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(prompts), batch_size), desc="æ‰¹é‡ç”Ÿæˆ"):
            batch_prompts = prompts[i:i + batch_size]
            
            # æ‰¹é‡ç¼–ç 
            inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512  # é™åˆ¶è¾“å…¥é•¿åº¦é˜²æ­¢æ˜¾å­˜ä¸è¶³
            ).to(self.model.device)
            
            with torch.no_grad():
                # æ‰¹é‡ç”Ÿæˆ
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    # å…³é”®ä¼˜åŒ–å‚æ•°
                    use_cache=True,  # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
                    num_beams=1,     # ä½¿ç”¨è´ªå©ªè§£ç åŠ é€Ÿ
                )
            
            # è§£ç å“åº”
            batch_responses = []
            for j, output in enumerate(outputs):
                response = self.tokenizer.decode(output, skip_special_tokens=True)
                # ç§»é™¤è¾“å…¥æç¤ºéƒ¨åˆ†
                original_prompt = batch_prompts[j]
                if response.startswith(original_prompt):
                    response = response[len(original_prompt):].strip()
                batch_responses.append(response)
            
            all_responses.extend(batch_responses)
            
            # æ¸…ç†æ˜¾å­˜
            del inputs, outputs
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
        return all_responses
            
    def evaluate_on_test_set(self, sample_size: int = 100, batch_size: int = 8):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼°æµ‹è¯•é›† (æ ·æœ¬æ•°: {sample_size}, æ‰¹é‡å¤§å°: {batch_size})...")
        
        # ç¡®ä¿ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
        self.load_finetuned_model()
        
        # ç¡®ä¿æ•°æ®å¤„ç†å™¨å·²åˆå§‹åŒ–
        if self.data_handler is None:
            self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
            # å¦‚æœéœ€è¦ï¼Œé‡æ–°åŠ è½½æ•°æ®
            if self.data_handler.test_data is None:
                self.data_handler.load_dataset()
        
        # è·å–æµ‹è¯•æ ·æœ¬
        test_samples = self.data_handler.get_test_samples(sample_size)
        logger.info(f"è·å–æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # æ‰¹é‡åˆ›å»ºæç¤º
        prompts = []
        for sample in test_samples:
            prompt = f"é—®é¢˜: {sample['question']}\nå›ç­”: "
            prompts.append(prompt)
        
        # æ‰¹é‡ç”Ÿæˆå“åº”
        logger.info("æ­£åœ¨æ‰¹é‡ç”Ÿæˆå“åº”...")
        self.model.eval()
        responses = self.generate_batch_responses(prompts, batch_size=batch_size)
        
        # å¤„ç†ç»“æœ
        results = []
        details = []
        correct = 0
        
        logger.info("æ­£åœ¨å¤„ç†ç»“æœ...")
        for i, (sample, response) in enumerate(tqdm(zip(test_samples, responses), desc="å¤„ç†ç»“æœ")):
            question = sample['question']
            ground_truth_text = sample['answer']
            ground_truth_answer = self.data_handler.extract_answer_from_text(ground_truth_text)
            
            predicted_answer = self.data_handler.extract_answer_from_text(response)
            
            # æ¯”è¾ƒç­”æ¡ˆ - æµ®ç‚¹æ•°æ¯”è¾ƒ
            is_correct = (predicted_answer is not None and 
                         ground_truth_answer is not None and 
                         abs(predicted_answer - ground_truth_answer) < 1e-6)
            
            if is_correct:
                correct += 1
            
            result = {
                'correct': is_correct,
            }
            results.append(result)
            
            details.append({
                'question': question,
                'ground_truth': ground_truth_answer,
                'predicted': predicted_answer,
                'correct': is_correct,
                'response': response
            })
            
            # æ‰“å°å‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
            if i < 5:
                logger.info(f"\n=== æµ‹è¯•æ ·æœ¬ {i+1} ===")
                logger.info(f"é—®é¢˜: {question}")
                logger.info(f"çœŸå®ç­”æ¡ˆ: {ground_truth_answer}")
                logger.info(f"é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
                logger.info(f"æ˜¯å¦æ­£ç¡®: {'âœ“' if is_correct else 'âœ—'}")
        
        total = len(results)
        accuracy = correct / total if total > 0 else 0
        
        logger.info(f"\n=== æ‰¹é‡æµ‹è¯•é›†è¯„ä¼°ç»“æœ ===")
        logger.info(f"æ€»æ ·æœ¬æ•°: {total}")
        logger.info(f"æ­£ç¡®é¢„æµ‹: {correct}")
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_results = {
            "total_samples": total,
            "correct_predictions": correct,
            "accuracy": accuracy,
            "evaluation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "batch_size": batch_size,
            "model_config": {
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "learning_rate": self.config.learning_rate,
                "num_epochs": self.config.num_epochs
            },
            "details": details
        }
        
        eval_path = os.path.join(self.config.output_dir, "batch_test_evaluation_results.json")
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_path}")
        
        return eval_results

def save_config(config: TrainingConfig, filepath: str):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config_dict = {
        'model_path': config.model_path,
        'dataset_path': config.dataset_path,
        'output_dir': config.output_dir,
        'max_length': config.max_length,
        'batch_size': config.batch_size,
        'gradient_accumulation_steps': config.gradient_accumulation_steps,
        'num_epochs': config.num_epochs,
        'learning_rate': config.learning_rate,
        'lora_rank': config.lora_rank,
        'lora_alpha': config.lora_alpha,
        'lora_dropout': config.lora_dropout,
        'target_modules': config.target_modules,
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

def load_config(filepath: str) -> TrainingConfig:
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    return TrainingConfig(**config_dict)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = TrainingConfig(
        model_path="/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B",
        dataset_path="/root/autodl-tmp/GRPO_MATH/gsm8k",
        output_dir="./lora_finetuned_qwen",
        max_length=512,
        batch_size=4,
        gradient_accumulation_steps=2,
        num_epochs=3,
        lora_rank=8,
        lora_alpha=32,
        lora_dropout=0.1
    )
    
    # ä¿å­˜é…ç½®
    os.makedirs(config.output_dir, exist_ok=True)
    save_config(config, os.path.join(config.output_dir, "training_config.json"))
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡Œè®­ç»ƒ
        trainer = QwenLoRATrainer(config)
        
        # æ‰§è¡Œè®­ç»ƒ
        logger.info("=" * 50)
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        logger.info("=" * 50)
        trainer.train()
        
        # è®­ç»ƒå®Œæˆåè¿›è¡Œæ‰¹é‡æµ‹è¯•é›†è¯„ä¼°
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æ‰¹é‡æµ‹è¯•é›†è¯„ä¼°")
        logger.info("=" * 50)
        eval_results = trainer.evaluate_on_test_set(sample_size=100, batch_size=16)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼æœ€ç»ˆç»“æœæ‘˜è¦ï¼š")
        logger.info("=" * 50)
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è·¯å¾„: {config.output_dir}")
        logger.info(f"ğŸ“Š æ‰¹é‡æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        logger.info(f"   - æµ‹è¯•æ ·æœ¬æ•°: {eval_results['total_samples']}")
        logger.info(f"   - æ­£ç¡®é¢„æµ‹æ•°: {eval_results['correct_predictions']}")
        logger.info(f"   - å‡†ç¡®ç‡: {eval_results['accuracy']:.3f} ({eval_results['accuracy']*100:.1f}%)")
        logger.info(f"ğŸ“„ è¯¦ç»†è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {os.path.join(config.output_dir, 'batch_test_evaluation_results.json')}")
        
        logger.info("=" * 50)
        logger.info("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_info = {
            "error_message": str(e),
            "error_type": type(e).__name__,
            "config": {
                "model_path": config.model_path,
                "dataset_path": config.dataset_path,
                "output_dir": config.output_dir,
                "batch_size": config.batch_size,
                "num_epochs": config.num_epochs,
                "lora_rank": config.lora_rank,
                "lora_alpha": config.lora_alpha,
                "lora_dropout": config.lora_dropout
            }
        }

        error_path = os.path.join(config.output_dir, "error_log.json")
        with open(error_path, 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {error_path}")
    
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†CUDAç¼“å­˜")

if __name__ == "__main__":
    main()