import os
import re
import json
import torch
import random
import numpy as np
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Union, Any
import logging
from datetime import datetime
from tqdm import tqdm
import transformers
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    HfArgumentParser,
    TrainingArguments,
    set_seed
)
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
from datasets import load_dataset, Dataset
from trl import PPOConfig, PPOTrainer, set_seed as trl_set_seed

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class RLTrainingConfig:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒé…ç½®ç±»"""
    # è·¯å¾„é…ç½®
    base_model_path: str = "/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B"
    lora_model_path: str = "./lora_finetuned_qwen"
    dataset_path: str = "/root/autodl-tmp/GRPO_MATH/gsm8k"
    output_dir: str = "./rl_finetuned_qwen"
    
    # æ•°æ®å‚æ•°
    max_length: int = 512
    train_val_split_ratio: float = 0.9
    
    # PPOè®­ç»ƒå‚æ•°
    batch_size: int = 2
    mini_batch_size: int = 1
    gradient_accumulation_steps: int = 4
    num_train_epochs: int = 3
    learning_rate: float = 5e-6
    lr_scheduler_type: str = "cosine"
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    
    # PPOç‰¹å®šå‚æ•°
    ppo_epochs: int = 4       # PPOæ›´æ–°æ¬¡æ•°
    init_kl_coef: float = 0.2 # KLæ•£åº¦åˆå§‹ç³»æ•°
    target_kl: float = 6.0    # ç›®æ ‡KLæ•£åº¦
    gamma: float = 0.99       # æŠ˜æ‰£å› å­
    lam: float = 0.95         # GAE-Lambda
    cliprange: float = 0.2    # PPOè£å‰ªå‚æ•°
    cliprange_value: float = 0.2  # ä»·å€¼å‡½æ•°è£å‰ªå‚æ•°
    vf_coef: float = 0.1      # ä»·å€¼æŸå¤±ç³»æ•°
    
    # å¥–åŠ±å‡½æ•°å‚æ•°
    correctness_reward: float = 10.0  # ç­”æ¡ˆæ­£ç¡®çš„å¥–åŠ±
    format_reward: float = 2.0  # æ ¼å¼æ­£ç¡®çš„å¥–åŠ±
    step_reward: float = 1.0  # æ¯ä¸ªæ¨ç†æ­¥éª¤çš„å¥–åŠ±
    
    # LoRAå‚æ•°
    lora_rank: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    target_modules: List[str] = field(default_factory=lambda: ["q_proj", "k_proj", "v_proj"])
    
    # å…¶ä»–é…ç½®
    use_bf16: bool = True
    logging_steps: int = 10
    save_steps: int = 100
    eval_steps: int = 100
    seed: int = 42
    
    def __post_init__(self):
        """åå¤„ç†é…ç½®"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)

class GSM8KDataHandler:
    """GSM8Kæ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, config: RLTrainingConfig, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
        self.train_data = None
        self.test_data = None
    
    def extract_answer_from_text(self, answer_text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å¯»æ‰¾ #### åé¢çš„æ•°å­—
        pattern = r'####\s*(-?\d+(?:\.\d+)?)'
        match = re.search(pattern, answer_text)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°####ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        
        return None
    
    def has_correct_format(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ­£ç¡®çš„æ ¼å¼"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å« #### æ ‡è®°
        has_marker = "####" in text
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªæ¨ç†æ­¥éª¤ï¼ˆè‡³å°‘æœ‰3è¡Œæ–‡æœ¬ï¼‰
        has_steps = len(text.split('\n')) >= 3
        
        return has_marker and has_steps
    
    def load_dataset(self) -> Tuple[Dataset, Dataset]:
        """åŠ è½½GSM8Kæ•°æ®é›†"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæ•°æ®é›†...")
        
        try:
            # åŠ è½½Parquetæ ¼å¼
            train_parquet_path = os.path.join(self.config.dataset_path, "main/train-00000-of-00001.parquet")
            test_parquet_path = os.path.join(self.config.dataset_path, "main/test-00000-of-00001.parquet")
            
            if os.path.exists(train_parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½æ•°æ®é›†")
                dataset = load_dataset("parquet", data_files={
                    "train": train_parquet_path,
                    "test": test_parquet_path
                })
                self.train_data = list(dataset['train'])
                self.test_data = list(dataset['test'])
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            raise
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(self.train_data)}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(self.test_data)}")
        
        # ä¸ºGRPOæ ¼å¼åŒ–æ•°æ®
        processed_train_data = self._preprocess_data_for_grpo(self.train_data)
        
        # åˆ›å»ºDatasetå¯¹è±¡å¹¶åˆ’åˆ†
        full_dataset = Dataset.from_list(processed_train_data)
        dataset_split = full_dataset.train_test_split(
            test_size=1 - self.config.train_val_split_ratio, 
            seed=self.config.seed
        )
        
        train_dataset = dataset_split['train']
        val_dataset = dataset_split['test']
        
        logger.info(f"åˆ’åˆ†åè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def _preprocess_data_for_grpo(self, raw_data: List[Dict]) -> List[Dict]:
        """ä¸ºGRPOé¢„å¤„ç†æ•°æ®"""
        processed_data = []
        
        for sample in tqdm(raw_data, desc="ä¸ºGRPOé¢„å¤„ç†æ•°æ®"):
            question = sample['question']
            answer = sample['answer']
            answer_label = self.extract_answer_from_text(answer)
            
            # æ ¼å¼åŒ–ä¸ºGRPOè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
            processed_sample = {
                "question": question,
                "answer": answer,
                "answer_label": answer_label,
                "prompt": f"Please solve this math problem step by step and provide your final answer after the \"####\" marker.\n{question}",
            }
            
            # è·³è¿‡æ— æ³•æå–ç­”æ¡ˆçš„æ ·æœ¬
            if answer_label is None:
                continue
                
            processed_data.append(processed_sample)
        
        logger.info(f"GRPOæ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°é‡: {len(processed_data)}")
        
        # å±•ç¤ºå‡ ä¸ªæ ·æœ¬
        for i in range(min(3, len(processed_data))):
            logger.info(f"\n=== æ ·æœ¬ {i+1} ===")
            logger.info(f"é—®é¢˜: {processed_data[i]['question']}")
            logger.info(f"ç­”æ¡ˆ: {processed_data[i]['answer']}")
            logger.info(f"ç­”æ¡ˆæ ‡ç­¾: {processed_data[i]['answer_label']}")
            logger.info(f"æç¤º: {processed_data[i]['prompt']}")
            
        return processed_data
    
    def get_test_samples(self, n_samples: Optional[int] = None) -> List[Dict]:
        """è·å–æµ‹è¯•æ ·æœ¬"""
        if self.test_data is None:
            raise ValueError("æµ‹è¯•æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_dataset()")
            
        if n_samples is None:
            return self.test_data
        return self.test_data[:min(n_samples, len(self.test_data))]

class RewardCalculator:
    """å¥–åŠ±è®¡ç®—å™¨ï¼Œç”¨äºè®¡ç®—æ¨¡å‹ç”Ÿæˆçš„å›ç­”çš„å¥–åŠ±"""
    
    def __init__(self, config: RLTrainingConfig, data_handler: GSM8KDataHandler):
        self.config = config
        self.data_handler = data_handler
    
    def compute_rewards(self, questions: List[str], 
                        generated_texts: List[str], 
                        ground_truth_answers: List[float]) -> List[float]:
        """è®¡ç®—ä¸€æ‰¹å›ç­”çš„å¥–åŠ±"""
        rewards = []
        
        for question, text, gt_answer in zip(questions, generated_texts, ground_truth_answers):
            # 1. è®¡ç®—æ­£ç¡®æ€§å¥–åŠ±
            predicted_answer = self.data_handler.extract_answer_from_text(text)
            correctness_reward = 0.0
            
            if predicted_answer is not None and gt_answer is not None:
                # å…è®¸å°çš„æ•°å€¼è¯¯å·®
                if abs(predicted_answer - gt_answer) < 1e-6:
                    correctness_reward = self.config.correctness_reward
                else:
                    # ç»™äºˆéƒ¨åˆ†å¥–åŠ±ï¼ŒåŸºäºç›¸å¯¹è¯¯å·®
                    relative_error = abs(predicted_answer - gt_answer) / max(1e-6, abs(gt_answer))
                    if relative_error < 0.1:  # è¯¯å·®å°äº10%
                        correctness_reward = self.config.correctness_reward * (1 - relative_error)
            
            # 2. è®¡ç®—æ ¼å¼å¥–åŠ±
            format_reward = 0.0
            if self.data_handler.has_correct_format(text):
                format_reward = self.config.format_reward
            
            # 3. è®¡ç®—æ­¥éª¤å¥–åŠ±
            step_reward = 0.0
            lines = [line for line in text.split('\n') if line.strip()]
            # æ¯ä¸ªéç©ºè¡Œè§†ä¸ºä¸€ä¸ªæ­¥éª¤ï¼Œä½†é™åˆ¶å¥–åŠ±ä¸Šé™
            step_count = min(len(lines) - 1, 5)  # å‡å»é—®é¢˜è¡Œï¼Œæœ€å¤šå¥–åŠ±5ä¸ªæ­¥éª¤
            if step_count > 0:
                step_reward = self.config.step_reward * step_count
            
            # è®¡ç®—æ€»å¥–åŠ±
            total_reward = correctness_reward + format_reward + step_reward
            rewards.append(total_reward)
            
        return rewards

class RLModelTrainer:
    """å¼ºåŒ–å­¦ä¹ æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: RLTrainingConfig):
        self.config = config
        self._setup_environment()
        self.tokenizer = None
        self.model = None
        self.ref_model = None  # å‚è€ƒæ¨¡å‹ï¼Œç”¨äºKLæ•£åº¦è®¡ç®—
        self.data_handler = None
        self.reward_calculator = None
        
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡å’Œéšæœºç§å­"""
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        set_seed(self.config.seed)
        trl_set_seed(self.config.seed)
        
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model_path}")
        logger.info(f"åŠ è½½LoRAé€‚é…å™¨: {self.config.lora_model_path}")
        
        # é¦–å…ˆå°è¯•ä»LoRAé€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨
        try:
            logger.info(f"å°è¯•ä»LoRAé€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨: {self.config.lora_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.lora_model_path,
                trust_remote_code=True, 
                padding_side='left'
            )
            logger.info("æˆåŠŸä»LoRAé€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨")
        except Exception as e:
            logger.warning(f"ä»LoRAé€‚é…å™¨ç›®å½•åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
            logger.info(f"ä»åŸºç¡€æ¨¡å‹åŠ è½½åˆ†è¯å™¨: {self.config.base_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.base_model_path, 
                trust_remote_code=True, 
                padding_side='left'
            )
    
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆç”¨äºKLæ•£åº¦è®¡ç®—ï¼‰
        self.ref_model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        try:
            peft_config = PeftConfig.from_pretrained(self.config.lora_model_path)
            self.model = PeftModel.from_pretrained(base_model, self.config.lora_model_path)
            logger.info(f"æˆåŠŸä» {self.config.lora_model_path} åŠ è½½LoRAé€‚é…å™¨")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ç°æœ‰LoRAé€‚é…å™¨: {e}ï¼Œåˆ›å»ºæ–°çš„LoRAé…ç½®")
            # å¦‚æœæ— æ³•åŠ è½½ï¼Œåˆ›å»ºæ–°çš„LoRAé…ç½®
            lora_config = LoraConfig(
                r=self.config.lora_rank,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=self.config.target_modules,
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(base_model, lora_config)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
    def prepare_datasets(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        train_dataset, val_dataset = self.data_handler.load_dataset()
        
        # åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨
        self.reward_calculator = RewardCalculator(self.config, self.data_handler)
        
        return train_dataset, val_dataset
    
    def _collator(self, data):
        """æ•°æ®æ•´ç†å‡½æ•°ï¼Œç”¨äºæ‰¹å¤„ç†"""
        prompts = [d["prompt"] for d in data]
        questions = [d["question"] for d in data]
        answer_labels = [d["answer_label"] for d in data]
        
        return {
            "input_ids": prompts,
            "questions": questions,
            "answer_labels": answer_labels
        }
    
    def compute_reward(self, question_tensors, response_tensors, ground_truth_answers):
        """è®¡ç®—å¥–åŠ±"""
        batch_size = len(question_tensors)
        rewards = torch.zeros(batch_size, device=self.model.device)
        
        # è§£ç ç”Ÿæˆçš„å›ç­”
        questions = [self.tokenizer.decode(q, skip_special_tokens=True) for q in question_tensors]
        responses = [self.tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]
        
        # è®¡ç®—å¥–åŠ±
        reward_list = self.reward_calculator.compute_rewards(
            questions, 
            responses, 
            ground_truth_answers
        )
        
        # è½¬æ¢ä¸ºå¼ é‡
        for i, reward in enumerate(reward_list):
            rewards[i] = reward
            
        return rewards
    
    def train(self):
        """æ‰§è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ"""
        logger.info("å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. å‡†å¤‡æ•°æ®é›†
            train_dataset, val_dataset = self.prepare_datasets()
            
            # 3. è®¾ç½®PPOè®­ç»ƒå‚æ•°
            ppo_config = PPOConfig(
                model_name=self.config.base_model_path,
                learning_rate=self.config.learning_rate,
                mini_batch_size=self.config.mini_batch_size,
                batch_size=self.config.batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                ppo_epochs=self.config.ppo_epochs,
                init_kl_coef=self.config.init_kl_coef,
                target_kl=self.config.target_kl,
                gamma=self.config.gamma,
                lam=self.config.lam,
                cliprange=self.config.cliprange,
                cliprange_value=self.config.cliprange_value,
                vf_coef=self.config.vf_coef,
                seed=self.config.seed,
            )
            
            # 4. åˆ›å»ºPPOè®­ç»ƒå™¨
            trainer = PPOTrainer(
                config=ppo_config,
                model=self.model,
                ref_model=self.ref_model,
                tokenizer=self.tokenizer,
                dataset=train_dataset,
                data_collator=self._collator,
            )
            
            # 5. æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # 6. å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹PPOè®­ç»ƒ...")
            
            # PPOè®­ç»ƒå¾ªç¯
            for epoch in range(self.config.num_train_epochs):
                logger.info(f"å¼€å§‹epoch {epoch+1}/{self.config.num_train_epochs}")
                
                # å¯¹æ¯ä¸ªbatchè¿›è¡Œè®­ç»ƒ
                for batch_idx, batch in enumerate(trainer.dataloader):
                    # ä»batchè·å–æ•°æ®
                    query_tensors = batch["input_ids"]
                    questions = batch["questions"]
                    answer_labels = batch["answer_labels"]
                    
                    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
                    response_tensors = []
                    for query in query_tensors:
                        # å°†queryè½¬æ¢ä¸ºtensor
                        query_tensor = torch.tensor(query if isinstance(query, list) else [query], device=trainer.accelerator.device)
                        # ç”Ÿæˆå›ç­”
                        response = trainer.generate(
                            query_tensor, 
                            max_new_tokens=self.config.max_length//2,
                            do_sample=True,
                            temperature=0.7
                        )
                        response_tensors.append(response.squeeze())
                    
                    # è®¡ç®—å¥–åŠ±
                    rewards = self.compute_reward(query_tensors, response_tensors, answer_labels)
                    
                    # æ‰§è¡ŒPPOæ›´æ–°
                    stats = trainer.step(query_tensors, response_tensors, rewards)
                    
                    # è®°å½•è®­ç»ƒä¿¡æ¯
                    if batch_idx % 10 == 0:
                        logger.info(f"Epoch {epoch+1}, Batch {batch_idx}: reward={stats['ppo/mean_rewards']:.4f}, kl={stats['ppo/mean_kl']:.4f}")
                
                # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
                output_dir = os.path.join(self.config.output_dir, f"epoch_{epoch+1}")
                os.makedirs(output_dir, exist_ok=True)
                trainer.model.save_pretrained(output_dir)
                self.tokenizer.save_pretrained(output_dir)
                logger.info(f"Epoch {epoch+1} å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")
            
            # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            logger.info("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            trainer.model.save_pretrained(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            logger.info("å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise
        
    def evaluate_model(self, sample_size: int = 100):
        """è¯„ä¼°GRPOå¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½"""
        logger.info(f"å¼€å§‹è¯„ä¼°GRPOå¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ (æ ·æœ¬æ•°: {sample_size})...")
        
        # ç¡®ä¿æ•°æ®å¤„ç†å™¨å·²åˆå§‹åŒ–
        if self.data_handler is None:
            self.data_handler = GSM8KDataHandler(self.config, self.tokenizer)
            # åŠ è½½æ•°æ®
            if self.data_handler.test_data is None:
                self.data_handler.load_dataset()
        
        # è·å–æµ‹è¯•æ ·æœ¬
        test_samples = self.data_handler.get_test_samples(sample_size)
        logger.info(f"è·å–æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_samples)}")
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        prompts = []
        for sample in test_samples:
            prompt = f"Please solve this math problem step by step and provide your final answer after the \"####\" marker.\n{sample['question']}"
            prompts.append(prompt)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # å¤„ç†ç»“æœ
        results = []
        correct = 0
        format_correct = 0
        
        # æ‰¹é‡å¤„ç†ç”Ÿæˆ
        for i in tqdm(range(0, len(prompts), 8), desc="è¯„ä¼°æ¨¡å‹"):
            batch_prompts = prompts[i:i+8]
            batch_samples = test_samples[i:i+8]
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=self.config.max_length // 2  # é¢„ç•™ä¸€åŠé•¿åº¦ç»™ç”Ÿæˆ
            ).to(self.model.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.max_length // 2,
                    temperature=0.1,
                    do_sample=False,  # è¯„ä¼°ä½¿ç”¨è´ªå©ªè§£ç 
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                )
            
            # è§£ç å›ç­”
            for j, output in enumerate(outputs):
                sample = batch_samples[j]
                response = self.tokenizer.decode(output, skip_special_tokens=True)
                
                # ç§»é™¤æç¤ºéƒ¨åˆ†
                original_prompt = batch_prompts[j]
                if response.startswith(original_prompt):
                    response = response[len(original_prompt):].strip()
                
                # æå–ç­”æ¡ˆ
                predicted_answer = self.data_handler.extract_answer_from_text(response)
                ground_truth_answer = self.data_handler.extract_answer_from_text(sample['answer'])
                
                # æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
                has_correct_format = self.data_handler.has_correct_format(response)
                if has_correct_format:
                    format_correct += 1
                
                # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
                is_correct = (predicted_answer is not None and 
                             ground_truth_answer is not None and 
                             abs(predicted_answer - ground_truth_answer) < 1e-6)
                
                if is_correct:
                    correct += 1
                
                # è®°å½•ç»“æœ
                results.append({
                    'question': sample['question'],
                    'ground_truth': ground_truth_answer,
                    'predicted': predicted_answer,
                    'correct': is_correct,
                    'format_correct': has_correct_format,
                    'response': response
                })
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                if i == 0 and j < 2:
                    logger.info(f"\n{'='*20} æµ‹è¯•æ ·æœ¬ {j+1} {'='*20}")
                    logger.info(f"é—®é¢˜: {sample['question']}")
                    logger.info(f"æ¨¡å‹å›ç­”:\n{response}")
                    logger.info(f"é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
                    logger.info(f"çœŸå®ç­”æ¡ˆ: {ground_truth_answer}")
                    logger.info(f"ç­”æ¡ˆæ­£ç¡®: {'âœ…' if is_correct else 'âŒ'}")
                    logger.info(f"æ ¼å¼æ­£ç¡®: {'âœ…' if has_correct_format else 'âŒ'}")
            
            # æ¸…ç†æ˜¾å­˜
            del inputs, outputs
            torch.cuda.empty_cache()
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        total = len(results)
        accuracy = correct / total if total > 0 else 0
        format_accuracy = format_correct / total if total > 0 else 0
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        logger.info("\n" + "="*50)
        logger.info("GRPOå¾®è°ƒåçš„æ¨¡å‹è¯„ä¼°ç»“æœ")
        logger.info("="*50)
        logger.info(f"æ€»æ ·æœ¬æ•°: {total}")
        logger.info(f"ç­”æ¡ˆæ­£ç¡®æ•°: {correct}")
        logger.info(f"æ ¼å¼æ­£ç¡®æ•°: {format_correct}")
        logger.info(f"ç­”æ¡ˆå‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.1f}%)")
        logger.info(f"æ ¼å¼å‡†ç¡®ç‡: {format_accuracy:.4f} ({format_accuracy*100:.1f}%)")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_results = {
            "total_samples": total,
            "correct_predictions": correct,
            "format_correct": format_correct,
            "answer_accuracy": accuracy,
            "format_accuracy": format_accuracy,
            "evaluation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "details": results
        }
        
        eval_path = os.path.join(self.config.output_dir, "grpo_evaluation_results.json")
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_path}")
        
        return eval_results

def save_config(config: RLTrainingConfig, filepath: str):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('_')}
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = RLTrainingConfig(
        base_model_path="/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B",
        lora_model_path="./lora_finetuned_qwen",
        dataset_path="/root/autodl-tmp/GRPO_MATH/gsm8k",
        output_dir="./rl_finetuned_qwen",
        
        # PPOè¶…å‚æ•°
        batch_size=2,
        mini_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        learning_rate=5e-6,
        ppo_epochs=4,
        init_kl_coef=0.2,
        
        # å¥–åŠ±å‡½æ•°æƒé‡
        correctness_reward=10.0,
        format_reward=2.0,
        step_reward=1.0
    )
    
    # ä¿å­˜é…ç½®
    os.makedirs(config.output_dir, exist_ok=True)
    save_config(config, os.path.join(config.output_dir, "rl_config.json"))
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RLModelTrainer(config)
        
        # æ‰§è¡Œè®­ç»ƒ
        logger.info("="*50)
        logger.info("å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
        logger.info("="*50)
        trainer.train()
        
        # è®­ç»ƒå®Œæˆåè¯„ä¼°æ¨¡å‹
        logger.info("="*50)
        logger.info("å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆï¼Œå¼€å§‹è¯„ä¼°æ¨¡å‹")
        logger.info("="*50)
        eval_results = trainer.evaluate_model(sample_size=100)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        logger.info("="*50)
        logger.info("å¼ºåŒ–å­¦ä¹ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼æœ€ç»ˆç»“æœæ‘˜è¦ï¼š")
        logger.info("="*50)
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è·¯å¾„: {config.output_dir}")
        logger.info(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        logger.info(f"   - æµ‹è¯•æ ·æœ¬æ•°: {eval_results['total_samples']}")
        logger.info(f"   - ç­”æ¡ˆæ­£ç¡®æ•°: {eval_results['correct_predictions']}")
        logger.info(f"   - ç­”æ¡ˆå‡†ç¡®ç‡: {eval_results['answer_accuracy']:.4f} ({eval_results['answer_accuracy']*100:.1f}%)")
        logger.info(f"   - æ ¼å¼æ­£ç¡®æ•°: {eval_results['format_correct']}")
        logger.info(f"   - æ ¼å¼å‡†ç¡®ç‡: {eval_results['format_accuracy']:.4f} ({eval_results['format_accuracy']*100:.1f}%)")
        logger.info(f"ğŸ“„ è¯¦ç»†è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {os.path.join(config.output_dir, 'rl_evaluation_results.json')}")
        
        logger.info("="*50)
        logger.info("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        logger.info("="*50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†CUDAç¼“å­˜")

if __name__ == "__main__":
    main()
