import os
import re
import json
import torch
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Union
import logging
from datetime import datetime
from tqdm import tqdm  # æ·»åŠ tqdmå¯¼å…¥
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    Trainer, 
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, PeftModel
from datasets import load_dataset, Dataset

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # è·¯å¾„é…ç½®
    model_path: str = "/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B"
    dataset_path: str = "/root/autodl-tmp/GRPO_MATH/gsm8k"
    output_dir: str = "./lora_finetuned_qwen"
    
    # è®­ç»ƒå‚æ•°
    max_length: int = 512
    batch_size: int = 4
    gradient_accumulation_steps: int = 2
    num_epochs: int = 3
    learning_rate: float = 2e-4
    warmup_steps: int = 100
    weight_decay: float = 0.01
    train_val_split_ratio: float = 0.8
    
    # LoRAå‚æ•°
    lora_rank: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: List[str] = None
    
    # å…¶ä»–é…ç½®
    use_bf16: bool = True
    logging_steps: int = 10
    
    def __post_init__(self):
        if self.target_modules is None:
            # self.target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
            self.target_modules = ["q_proj", "v_proj"]

class GSM8KDataHandler:
    """GSM8Kæ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, config: TrainingConfig, tokenizer):
        self.config = config
        self.tokenizer = tokenizer
    
    def extract_answer_from_text(self, answer_text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å¯»æ‰¾ #### åé¢çš„æ•°å­—
        pattern = r'####\s*(-?\d+(?:\.\d+)?)'
        match = re.search(pattern, answer_text)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        # å¦‚æœæ²¡æ‰¾åˆ°####ï¼Œå¯»æ‰¾æœ€åä¸€ä¸ªæ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            try:
                return float(numbers[-1])
            except ValueError:
                pass
        
        return None
        
    def load_dataset(self) -> Tuple[Dataset, Dataset]:
        """åŠ è½½GSM8Kæ•°æ®é›†"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæ•°æ®é›†...")
        
        try:
            # å°è¯•åŠ è½½Parquetæ ¼å¼
            parquet_path = os.path.join(self.config.dataset_path, "main/train-00000-of-00001.parquet")
            if os.path.exists(parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½: {parquet_path}")
                dataset = load_dataset("parquet", data_files={"train": parquet_path})
                train_data = list(dataset['train'])
            else:
                # å°è¯•å…¶ä»–æ ¼å¼
                logger.info("å°è¯•ä»ç›®å½•åŠ è½½æ•°æ®é›†...")
                dataset = load_dataset(self.config.dataset_path)
                train_data = list(dataset['train'])
                
        except Exception as e:
            logger.error(f"åŠ è½½æœ¬åœ°æ•°æ®é›†å¤±è´¥: {e}")
            logger.info("å°è¯•ä»HuggingFaceåŠ è½½GSM8K...")
            dataset = load_dataset("gsm8k", "default")
            train_data = list(dataset['train'])
        
        logger.info(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(train_data)}")
        
        # æ•°æ®é¢„å¤„ç†
        processed_data = self._preprocess_data(train_data)
        
        # åˆ›å»ºDatasetå¯¹è±¡å¹¶åˆ’åˆ†
        full_dataset = Dataset.from_list(processed_data)
        dataset_split = full_dataset.train_test_split(
            test_size=1 - self.config.train_val_split_ratio, 
            seed=42
        )
        
        train_dataset = dataset_split['train']
        val_dataset = dataset_split['test']
        
        logger.info(f"åˆ’åˆ†åè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        return train_dataset, val_dataset
    
    def load_test_dataset(self) -> Dataset:
        """åŠ è½½GSM8Kæµ‹è¯•é›†"""
        logger.info("å¼€å§‹åŠ è½½GSM8Kæµ‹è¯•é›†...")
        
        try:
            # åŠ è½½æµ‹è¯•é›†Parquetæ–‡ä»¶
            test_parquet_path = os.path.join(self.config.dataset_path, "main/test-00000-of-00001.parquet")
            if os.path.exists(test_parquet_path):
                logger.info(f"ä»Parquetæ–‡ä»¶åŠ è½½æµ‹è¯•é›†: {test_parquet_path}")
                dataset = load_dataset("parquet", data_files={"test": test_parquet_path})
                test_data = list(dataset['test'])
            else:
                # å°è¯•ä»HuggingFaceåŠ è½½
                logger.info("å°è¯•ä»HuggingFaceåŠ è½½GSM8Kæµ‹è¯•é›†...")
                dataset = load_dataset("gsm8k", "default")
                test_data = list(dataset['test'])
                
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•é›†å¤±è´¥: {e}")
            return None
        
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
        
        # é¢„å¤„ç†æµ‹è¯•æ•°æ®
        processed_test_data = self._preprocess_data(test_data)
        test_dataset = Dataset.from_list(processed_test_data)
        
        return test_dataset
    
    def _preprocess_data(self, raw_data: List[Dict]) -> List[Dict]:
        """é¢„å¤„ç†åŸå§‹æ•°æ®ï¼Œæå–ç­”æ¡ˆæ ‡ç­¾"""
        processed_data = []
        
        for sample in raw_data:
            # æå–æœ€ç»ˆç­”æ¡ˆæ•°å­—
            answer_label = self.extract_answer_from_text(sample['answer'])
            
            # æ„é€ è¾“å…¥æ–‡æœ¬ï¼Œä¿æŒCoTæ ¼å¼
            input_text = f"é—®é¢˜: {sample['question']}\nå›ç­”: {sample['answer']}"
            
            processed_data.append({
                "text": input_text,
                "answer": sample['answer'],
                "answer_label": answer_label,
                "question": sample['question']
            })
        
        logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæ ·æœ¬æ•°é‡: {len(processed_data)}")
        # # æ‰“å°å‡ ä¸ªæ ·æœ¬çš„ç­”æ¡ˆæå–ç»“æœ
        # for i in range(min(3, len(processed_data))):
        #     logger.info(f"æ ·æœ¬{i+1} åŸå§‹ç­”æ¡ˆ: {processed_data[i]['answer'][:50]}...")
        #     logger.info(f"æ ·æœ¬{i+1} æå–ç­”æ¡ˆ: {processed_data[i]['answer_label']}")
        
        return processed_data
    
    def tokenize_dataset(self, dataset: Dataset) -> Dataset:
        """å¯¹æ•°æ®é›†è¿›è¡Œtokenization"""
        logger.info("å¼€å§‹tokenization...")
        
        def tokenize_function(examples):
            # ç¼–ç å®Œæ•´æ–‡æœ¬ï¼ˆé—®é¢˜+å›ç­”ï¼‰
            encodings = self.tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=self.config.max_length,
                return_tensors="pt"
            )
            
            # ç¼–ç ç­”æ¡ˆéƒ¨åˆ†ä½œä¸ºlabels
            answer_encodings = self.tokenizer(
                examples["answer"],
                padding="max_length",
                truncation=True,
                max_length=self.config.max_length,
                return_tensors="pt"
            )
            
            # è®¾ç½®labelsï¼Œå¿½ç•¥paddingéƒ¨åˆ†
            encodings["labels"] = answer_encodings["input_ids"].clone()
            encodings["labels"][answer_encodings["attention_mask"] == 0] = -100
            
            return encodings
        
        # åº”ç”¨tokenization
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True, 
            num_proc=1,
            desc="Tokenizing dataset"
        )
        
        # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—å¹¶è®¾ç½®æ ¼å¼
        tokenized_dataset = tokenized_dataset.remove_columns([col for col in dataset.column_names if col not in ["input_ids", "attention_mask", "labels"]])
        tokenized_dataset.set_format("torch")
        
        return tokenized_dataset

class QwenLoRATrainer:
    """Qwen LoRAå¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self._setup_environment()
        self.tokenizer = None
        self.model = None
        self.trainer = None
        self.original_model = None  # ä¿å­˜åŸå§‹æ¨¡å‹çš„å¼•ç”¨
        
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡"""
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: {self.config.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.original_model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
        )
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {self.original_model.num_parameters():,}")
        
    def setup_lora(self):
        """é…ç½®LoRA"""
        logger.info("é…ç½®LoRAå‚æ•°...")
        
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.target_modules,
            task_type="CAUSAL_LM"
        )
        
        self.model = get_peft_model(self.original_model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
    def prepare_datasets(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        
        # åŠ è½½åŸå§‹æ•°æ®é›†
        train_dataset, val_dataset = data_handler.load_dataset()
        
        # tokenization
        train_dataset = data_handler.tokenize_dataset(train_dataset)
        val_dataset = data_handler.tokenize_dataset(val_dataset)
        
        return train_dataset, val_dataset
        
    def setup_trainer(self, train_dataset: Dataset, val_dataset: Dataset):
        """è®¾ç½®è®­ç»ƒå™¨"""
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            weight_decay=self.config.weight_decay,
            logging_dir=os.path.join(self.config.output_dir, "logs"),
            logging_steps=self.config.logging_steps,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=not self.config.use_bf16,
            bf16=self.config.use_bf16,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
        )
        
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
        )
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†"""
        logger.info("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†...")
        
        # å¦‚æœå·²ç»æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨å®ƒ
        if self.model is not None:
            logger.info("ä½¿ç”¨å½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
        
        # å¦åˆ™ä»ä¿å­˜çš„è·¯å¾„åŠ è½½
        if os.path.exists(self.config.output_dir):
            logger.info(f"ä» {self.config.output_dir} åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
            
            # é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path, trust_remote_code=True, padding_side='left')
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            base_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            self.model = PeftModel.from_pretrained(base_model, self.config.output_dir)
            logger.info("å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ")
        else:
            raise ValueError(f"æ‰¾ä¸åˆ°å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„: {self.config.output_dir}")
        
    def train(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. é…ç½®LoRA
            self.setup_lora()
            
            # 3. å‡†å¤‡æ•°æ®é›†
            train_dataset, val_dataset = self.prepare_datasets()
            
            # 4. è®¾ç½®è®­ç»ƒå™¨
            self.setup_trainer(train_dataset, val_dataset)
            
            # 5. æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            
            # 6. å¼€å§‹è®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒ...")
            self.trainer.train()
            
            # 7. ä¿å­˜æ¨¡å‹
            logger.info("ä¿å­˜æ¨¡å‹...")
            self.model.save_pretrained(self.config.output_dir)
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            logger.info("è®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
            
    def generate_answer(self, question: str) -> Dict[str, Union[str, Optional[float]]]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("æ¨¡å‹å’Œåˆ†è¯å™¨æœªåŠ è½½ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæˆ–åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
            
        input_text = f"é—®é¢˜: {question}\nå›ç­”: "
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs, 
                max_length=self.config.max_length,
                num_return_sequences=1,
                temperature=0.1,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–æœ€ç»ˆç­”æ¡ˆ - ä½¿ç”¨ä¿®æ”¹åçš„å‡½æ•°
        data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        final_answer = data_handler.extract_answer_from_text(generated_text)
        
        return {
            "generated_text": generated_text,
            "final_answer": final_answer
        }
    
    def evaluate_on_test_set(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("å¼€å§‹åœ¨GSM8Kæµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        
        # ç¡®ä¿ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
        self.load_finetuned_model()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        data_handler = GSM8KDataHandler(self.config, self.tokenizer)
        test_dataset = data_handler.load_test_dataset()
        
        if test_dataset is None:
            logger.error("æ— æ³•åŠ è½½æµ‹è¯•é›†")
            return
        
        self.model.eval()
        correct = 0
        total = 0
        
        # å–å‰100ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        test_samples = test_dataset.select(range(min(100, len(test_dataset))))
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        progress_bar = tqdm(test_samples, desc="è¯„ä¼°æµ‹è¯•é›†", unit="æ ·æœ¬")
        
        for sample in progress_bar:
            total += 1
            question = sample['question']
            true_answer = sample['answer_label']
            
            try:
                result = self.generate_answer(question)
                predicted_answer = result['final_answer']
                
                # æ¯”è¾ƒç­”æ¡ˆ - ç°åœ¨æ¯”è¾ƒçš„æ˜¯æµ®ç‚¹æ•°
                if predicted_answer is not None and true_answer is not None:
                    # ä½¿ç”¨æµ®ç‚¹æ•°æ¯”è¾ƒï¼Œå…è®¸å°çš„è¯¯å·®
                    if abs(predicted_answer - true_answer) < 1e-6:
                        correct += 1
                elif predicted_answer == true_answer:  # éƒ½æ˜¯Noneçš„æƒ…å†µ
                    correct += 1
                
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰å‡†ç¡®ç‡
                current_accuracy = correct / total if total > 0 else 0
                progress_bar.set_postfix({
                    "å‡†ç¡®ç‡": f"{current_accuracy:.3f}",
                    "æ­£ç¡®/æ€»è®¡": f"{correct}/{total}"
                })
                
                if total <= 5:  # æ‰“å°å‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
                    logger.info(f"\n=== æµ‹è¯•æ ·æœ¬ {total} ===")
                    logger.info(f"é—®é¢˜: {question}")
                    logger.info(f"çœŸå®ç­”æ¡ˆ: {true_answer}")
                    logger.info(f"é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
                    is_correct = (predicted_answer is not None and true_answer is not None and 
                                abs(predicted_answer - true_answer) < 1e-6) or (predicted_answer == true_answer)
                    logger.info(f"æ˜¯å¦æ­£ç¡®: {'âœ“' if is_correct else 'âœ—'}")
                
            except Exception as e:
                logger.error(f"è¯„ä¼°æ ·æœ¬ {total} æ—¶å‡ºé”™: {e}")
        
        progress_bar.close()
        
        accuracy = correct / total if total > 0 else 0
        logger.info(f"\n=== æµ‹è¯•é›†è¯„ä¼°ç»“æœ ===")
        logger.info(f"æ€»æ ·æœ¬æ•°: {total}")
        logger.info(f"æ­£ç¡®é¢„æµ‹: {correct}")
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_results = {
            "total_samples": total,
            "correct_predictions": correct,
            "accuracy": accuracy,
            "evaluation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "model_config": {
                "lora_rank": self.config.lora_rank,
                "lora_alpha": self.config.lora_alpha,
                "learning_rate": self.config.learning_rate,
                "num_epochs": self.config.num_epochs
            }
        }
        
        eval_path = os.path.join(self.config.output_dir, "test_evaluation_results.json")
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_path}")
        
        return eval_results

def save_config(config: TrainingConfig, filepath: str):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config_dict = {
        'model_path': config.model_path,
        'dataset_path': config.dataset_path,
        'output_dir': config.output_dir,
        'max_length': config.max_length,
        'batch_size': config.batch_size,
        'gradient_accumulation_steps': config.gradient_accumulation_steps,
        'num_epochs': config.num_epochs,
        'learning_rate': config.learning_rate,
        'lora_rank': config.lora_rank,
        'lora_alpha': config.lora_alpha,
        'lora_dropout': config.lora_dropout,
        'target_modules': config.target_modules,
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)

def load_config(filepath: str) -> TrainingConfig:
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    return TrainingConfig(**config_dict)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = TrainingConfig(
        model_path="/root/autodl-tmp/GRPO_MATH/Qwen2_0.5B",
        dataset_path="/root/autodl-tmp/GRPO_MATH/gsm8k",
        output_dir="./lora_finetuned_qwen",
        max_length=512,
        batch_size=4,
        gradient_accumulation_steps=2,
        num_epochs=3,
        lora_rank=8,
        lora_alpha=32,
        lora_dropout=0.1
    )
    
    # ä¿å­˜é…ç½®
    os.makedirs(config.output_dir, exist_ok=True)
    save_config(config, os.path.join(config.output_dir, "training_config.json"))
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡Œè®­ç»ƒ
        trainer = QwenLoRATrainer(config)
        
        # æ‰§è¡Œè®­ç»ƒ
        logger.info("=" * 50)
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        logger.info("=" * 50)
        trainer.train()
        
        # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•é›†è¯„ä¼°
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå®Œæˆï¼Œå¼€å§‹æµ‹è¯•é›†è¯„ä¼°")
        logger.info("=" * 50)
        eval_results = trainer.evaluate_on_test_set()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ‘˜è¦
        logger.info("=" * 50)
        logger.info("è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼æœ€ç»ˆç»“æœæ‘˜è¦ï¼š")
        logger.info("=" * 50)
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è·¯å¾„: {config.output_dir}")
        logger.info(f"ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        logger.info(f"   - æµ‹è¯•æ ·æœ¬æ•°: {eval_results['total_samples']}")
        logger.info(f"   - æ­£ç¡®é¢„æµ‹æ•°: {eval_results['correct_predictions']}")
        logger.info(f"   - å‡†ç¡®ç‡: {eval_results['accuracy']:.3f} ({eval_results['accuracy']*100:.1f}%)")
        logger.info(f"ğŸ“„ è¯¦ç»†è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {os.path.join(config.output_dir, 'test_evaluation_results.json')}")
        
        
        
        logger.info("=" * 50)
        logger.info("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å³ä½¿è®­ç»ƒå¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿å­˜ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯
        error_info = {
            "error_message": str(e),
            "error_type": type(e).__name__,
            "config": {
                "model_path": config.model_path,
                "dataset_path": config.dataset_path,
                "output_dir": config.output_dir,
                "batch_size": config.batch_size,
                "num_epochs": config.num_epochs,
                "lora_rank": config.lora_rank,
                "lora_alpha": config.lora_alpha,
                "lora_dropout": config.lora_dropout
            }
        }

        error_path = os.path.join(config.output_dir, "error_log.json")
        with open(error_path, 'w', encoding='utf-8') as f:
            json.dump(error_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {error_path}")
    
    finally:
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("å·²æ¸…ç†CUDAç¼“å­˜")

if __name__ == "__main__":
    main()